#!/usr/bin/env python3
"""
Skrypt do dog≈Çƒôbnego por√≥wnania danych z pliku electricity_production_entsoe_all (2).csv
z danymi pobieranymi przez system z PSE i ENTSO-E.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sys
import os

# Dodaj ≈õcie≈ºkƒô do modu≈Ç√≥w
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from pse_energy_scraper import PSEEnergyDataFetcher
from entsoe_data_fetcher import ENTSOEDataFetcher


def load_csv_file(filepath):
    """Wczytuje plik CSV i przygotowuje do analizy."""
    print("üìÇ Wczytywanie pliku CSV...")
    df = pd.read_csv(filepath)
    
    # Informacje o pliku
    print(f"   Liczba wierszy: {len(df)}")
    print(f"   Kolumny: {list(df.columns)}")
    
    # Parsowanie dat
    if 'date' in df.columns:
        df['date_parsed'] = pd.to_datetime(df['date'], format='%d.%m.%Y %H:%M')
        print(f"   Zakres dat (data lokalna): {df['date_parsed'].min()} - {df['date_parsed'].max()}")
    
    if 'date_utc' in df.columns:
        df['date_utc_parsed'] = pd.to_datetime(df['date_utc'], format='%d.%m.%Y %H:%M')
        print(f"   Zakres dat (UTC): {df['date_utc_parsed'].min()} - {df['date_utc_parsed'].max()}")
    
    return df


def analyze_csv_structure(df):
    """Analizuje strukturƒô i jako≈õƒá danych w pliku CSV."""
    print("\n" + "="*80)
    print("üìä ANALIZA STRUKTURY PLIKU CSV")
    print("="*80)
    
    # Statystyki podstawowe
    print(f"\n1. PODSTAWOWE INFORMACJE:")
    print(f"   - Liczba rekord√≥w: {len(df):,}")
    print(f"   - Liczba kolumn: {len(df.columns)}")
    print(f"   - Rozmiar w pamiƒôci: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Zakres czasowy
    if 'date_parsed' in df.columns:
        min_date = df['date_parsed'].min()
        max_date = df['date_parsed'].max()
        date_range_days = (max_date - min_date).days
        
        print(f"\n2. ZAKRES CZASOWY:")
        print(f"   - Od: {min_date}")
        print(f"   - Do: {max_date}")
        print(f"   - Okres: {date_range_days} dni ({date_range_days / 365.25:.1f} lat)")
        print(f"   - Oczekiwana liczba godzin: {date_range_days * 24:,}")
        print(f"   - Faktyczna liczba rekord√≥w: {len(df):,}")
        print(f"   - R√≥≈ºnica: {abs(date_range_days * 24 - len(df)):,} godzin")
    
    # BrakujƒÖce warto≈õci
    print(f"\n3. BRAKUJƒÑCE WARTO≈öCI:")
    missing_cols = {}
    for col in df.columns:
        if col not in ['date', 'date_utc', 'date_parsed', 'date_utc_parsed']:
            missing = df[col].isna().sum()
            if missing > 0:
                missing_cols[col] = missing
    
    if missing_cols:
        for col, count in sorted(missing_cols.items(), key=lambda x: x[1], reverse=True):
            pct = (count / len(df)) * 100
            print(f"   - {col}: {count:,} ({pct:.1f}%)")
    else:
        print("   ‚úì Brak brakujƒÖcych warto≈õci")
    
    # Typy ≈∫r√≥de≈Ç energii
    energy_sources = [col for col in df.columns if col not in 
                     ['date', 'date_utc', 'date_parsed', 'date_utc_parsed']]
    
    print(f"\n4. ≈πR√ìD≈ÅA ENERGII ({len(energy_sources)} typ√≥w):")
    for source in energy_sources:
        if pd.api.types.is_numeric_dtype(df[source]):
            stats = df[source].describe()
            print(f"   - {source}:")
            print(f"     Min: {stats['min']:.2f} MW")
            print(f"     Max: {stats['max']:.2f} MW")
            print(f"     ≈örednia: {stats['mean']:.2f} MW")
            print(f"     Suma: {df[source].sum():.2f} MWh (za ca≈Çy okres)")
    
    return energy_sources


def fetch_comparison_data(date_from, date_to, sample_size=7):
    """
    Pobiera dane z PSE i ENTSO-E dla wybranego okresu.
    Dla d≈Çugich okres√≥w pobiera tylko pr√≥bki.
    """
    print("\n" + "="*80)
    print("üì• POBIERANIE DANYCH Z API")
    print("="*80)
    
    # Oblicz d≈Çugo≈õƒá okresu
    start_date = datetime.strptime(date_from, '%Y-%m-%d')
    end_date = datetime.strptime(date_to, '%Y-%m-%d')
    total_days = (end_date - start_date).days
    
    if total_days > sample_size * 5:
        print(f"\n‚ö†Ô∏è  Okres {total_days} dni jest zbyt d≈Çugi dla pe≈Çnego por√≥wnania.")
        print(f"   Pobieram pr√≥bki: {sample_size} dni z poczƒÖtku, ≈õrodka i ko≈Ñca okresu")
        
        # Pobierz pr√≥bki
        samples = []
        
        # PoczƒÖtek
        samples.append((start_date, start_date + timedelta(days=sample_size-1)))
        
        # ≈örodek
        mid_date = start_date + timedelta(days=total_days // 2)
        samples.append((mid_date, mid_date + timedelta(days=sample_size-1)))
        
        # Koniec
        end_sample_start = end_date - timedelta(days=sample_size-1)
        samples.append((end_sample_start, end_date))
        
        all_pse = []
        all_entsoe = []
        
        for sample_start, sample_end in samples:
            s_from = sample_start.strftime('%Y-%m-%d')
            s_to = sample_end.strftime('%Y-%m-%d')
            print(f"\nüìÖ Pobieranie pr√≥bki: {s_from} - {s_to}")
            
            pse_data, entsoe_data = _fetch_single_period(s_from, s_to)
            if pse_data is not None:
                all_pse.append(pse_data)
            if entsoe_data is not None:
                all_entsoe.append(entsoe_data)
        
        # Po≈ÇƒÖcz pr√≥bki
        df_pse = pd.concat(all_pse) if all_pse else None
        df_entsoe = pd.concat(all_entsoe) if all_entsoe else None
        
    else:
        # Pe≈Çny zakres dla kr√≥tkich okres√≥w
        print(f"\nüìÖ Pobieranie pe≈Çnych danych: {date_from} - {date_to}")
        df_pse, df_entsoe = _fetch_single_period(date_from, date_to)
    
    return df_pse, df_entsoe


def _fetch_single_period(date_from, date_to):
    """Pobiera dane dla pojedynczego okresu."""
    # PSE
    print(f"\nüîå Pobieranie z PSE...")
    pse_fetcher = PSEEnergyDataFetcher()
    df_pse = pse_fetcher.fetch_data(date_from, date_to)
    
    if df_pse is not None and not df_pse.empty:
        print(f"   ‚úì Pobrano {len(df_pse)} rekord√≥w z PSE")
    else:
        print(f"   ‚ö†Ô∏è Brak danych z PSE")
    
    # ENTSO-E
    print(f"\n‚ö° Pobieranie z ENTSO-E...")
    try:
        entsoe_fetcher = ENTSOEDataFetcher()
        df_entsoe = entsoe_fetcher.fetch_generation_data(date_from, date_to)
        
        if df_entsoe is not None and not df_entsoe.empty:
            print(f"   ‚úì Pobrano {len(df_entsoe)} rekord√≥w z ENTSO-E")
        else:
            print(f"   ‚ö†Ô∏è Brak danych z ENTSO-E")
    except Exception as e:
        print(f"   ‚ùå B≈ÇƒÖd ENTSO-E: {e}")
        df_entsoe = None
    
    return df_pse, df_entsoe


def compare_with_entsoe(df_csv, df_entsoe):
    """Por√≥wnuje dane z CSV z danymi z ENTSO-E API."""
    print("\n" + "="*80)
    print("‚ö° POR√ìWNANIE Z DANYMI ENTSO-E")
    print("="*80)
    
    if df_entsoe is None or df_entsoe.empty:
        print("‚ùå Brak danych z ENTSO-E do por√≥wnania")
        return
    
    # Przygotuj dane ENTSO-E
    if 'Data' in df_entsoe.columns:
        df_entsoe['timestamp'] = pd.to_datetime(df_entsoe['Data'])
    else:
        df_entsoe['timestamp'] = df_entsoe.index
    
    print(f"\n‚ÑπÔ∏è  Kolumny w danych ENTSO-E API: {list(df_entsoe.columns)}")
    print(f"   Zakres dat: {df_entsoe['timestamp'].min()} - {df_entsoe['timestamp'].max()}")
    print(f"   Liczba rekord√≥w: {len(df_entsoe)}")
    
    # Usu≈Ñ timezone z danych API dla por√≥wnania
    if df_entsoe['timestamp'].dt.tz is not None:
        print(f"   üîÑ Konwersja timezone API do naive datetime...")
        df_entsoe['timestamp'] = df_entsoe['timestamp'].dt.tz_localize(None)
    
    # Sprawd≈∫ czƒôstotliwo≈õƒá danych CSV
    csv_time_diffs = df_csv['date_parsed'].diff()
    csv_most_common_diff = csv_time_diffs.mode()[0] if len(csv_time_diffs.mode()) > 0 else pd.Timedelta(hours=1)
    
    if csv_most_common_diff == pd.Timedelta(minutes=15):
        print(f"\n   ‚ÑπÔ∏è  CSV ma dane 15-minutowe - por√≥wnanie bez agregacji")
        df_entsoe_comp = df_entsoe.copy()
        df_entsoe_comp['comp_time'] = df_entsoe_comp['timestamp']
        df_csv['comp_time'] = df_csv['date_parsed']
    else:
        # Agreguj dane ENTSO-E do pe≈Çnych godzin (≈õrednia z 4 pomiar√≥w po 15 min)
        print(f"\n   üîÑ Agregacja danych ENTSO-E z 15-minutowych do godzinowych...")
        df_entsoe['hour'] = df_entsoe['timestamp'].dt.floor('h')
        
        # Kolumny do agregacji (wszystkie MW kolumny)
        agg_cols = [col for col in df_entsoe.columns if '[MW]' in col and col != 'Data']
        
        df_entsoe_comp = df_entsoe.groupby('hour')[agg_cols].mean().reset_index()
        df_entsoe_comp.rename(columns={'hour': 'comp_time'}, inplace=True)
        df_csv['comp_time'] = df_csv['date_parsed'].dt.floor('h')
        print(f"   ‚úì Zagregowano do {len(df_entsoe_comp)} godzin")
    
    # Mapowanie kolumn CSV -> ENTSO-E
    column_mapping = {
        'hard_coal': 'Wƒôgiel kamienny [MW]',
        'lignite': 'Wƒôgiel brunatny [MW]',
        'gas': 'Gaz [MW]',
        'biomass': 'Biomasa [MW]',
        'wind_onshore': 'Wiatr lƒÖdowy [MW]',
        'solar': 'S≈Ço≈Ñce [MW]',
        'hydro_pumped_storage': 'Magazyny energii [MW]',
        'hydro_run-of-river_and_poundage': 'Woda (przep≈Çywowa) [MW]',
        'hydro_water_reservoir': 'Woda (zbiornikowa) [MW]'
    }
    
    print("\n1. POR√ìWNANIE ≈πR√ìDE≈Å ENERGII:")
    print("-" * 80)
    
    differences_found = []
    
    for csv_col, entsoe_col in column_mapping.items():
        if csv_col not in df_csv.columns:
            print(f"‚ö†Ô∏è  {csv_col}: brak w pliku CSV")
            continue
        
        if entsoe_col not in df_entsoe_comp.columns:
            print(f"‚ö†Ô∏è  {entsoe_col}: brak w danych ENTSO-E API")
            continue
        
        # Znajd≈∫ wsp√≥lne daty
        csv_dates = set(df_csv['comp_time'])
        entsoe_dates = set(df_entsoe_comp['comp_time'])
        common_dates = csv_dates.intersection(entsoe_dates)
        
        if len(common_dates) == 0:
            print(f"‚ö†Ô∏è  {csv_col} vs {entsoe_col}: brak wsp√≥lnych dat")
            print(f"     CSV min/max: {df_csv['comp_time'].min()} / {df_csv['comp_time'].max()}")
            print(f"     API min/max: {df_entsoe_comp['comp_time'].min()} / {df_entsoe_comp['comp_time'].max()}")
            continue
        
        # Przygotuj dane do por√≥wnania
        df_csv_subset = df_csv[df_csv['comp_time'].isin(common_dates)].copy()
        df_entsoe_subset = df_entsoe_comp[df_entsoe_comp['comp_time'].isin(common_dates)].copy()
        
        # Merge na czas
        merged = pd.merge(
            df_csv_subset[['comp_time', csv_col]],
            df_entsoe_subset[['comp_time', entsoe_col]],
            on='comp_time',
            how='inner'
        )
        
        if len(merged) == 0:
            continue
        
        # Oblicz r√≥≈ºnice
        merged['diff'] = merged[csv_col] - merged[entsoe_col]
        merged['diff_pct'] = (merged['diff'] / merged[entsoe_col].replace(0, np.nan)) * 100
        
        # Statystyki
        mean_diff = merged['diff'].mean()
        max_diff = merged['diff'].abs().max()
        mean_pct_diff = merged['diff_pct'].abs().mean()
        
        # Korelacja
        correlation = merged[csv_col].corr(merged[entsoe_col])
        
        status = "‚úì" if abs(mean_diff) < 10 and correlation > 0.99 else "‚ö†Ô∏è"
        
        print(f"\n{status} {csv_col} vs {entsoe_col}:")
        print(f"   - Wsp√≥lnych pomiar√≥w: {len(merged)}")
        print(f"   - ≈örednia r√≥≈ºnica: {mean_diff:.2f} MW")
        print(f"   - Maksymalna r√≥≈ºnica: {max_diff:.2f} MW")
        print(f"   - ≈örednia r√≥≈ºnica %: {mean_pct_diff:.2f}%")
        print(f"   - Korelacja: {correlation:.6f}")
        
        if abs(mean_diff) >= 10 or correlation < 0.99:
            differences_found.append({
                'source': csv_col,
                'mean_diff': mean_diff,
                'max_diff': max_diff,
                'correlation': correlation
            })
            
            # Poka≈º przyk≈Çady najwiƒôkszych r√≥≈ºnic
            top_diffs = merged.nlargest(3, 'diff')[['comp_time', csv_col, entsoe_col, 'diff']]
            print(f"   Najwiƒôksze r√≥≈ºnice:")
            for _, row in top_diffs.iterrows():
                print(f"     {row['comp_time']}: CSV={row[csv_col]:.2f} MW, API={row[entsoe_col]:.2f} MW, diff={row['diff']:.2f} MW")
    
    # Podsumowanie
    print("\n" + "="*80)
    if differences_found:
        print(f"‚ö†Ô∏è  ZNALEZIONO {len(differences_found)} ≈πR√ìDE≈Å Z ISTOTNYMI R√ì≈ªNICAMI")
        for diff in differences_found:
            print(f"   - {diff['source']}: ≈õrednia r√≥≈ºnica {diff['mean_diff']:.2f} MW, korelacja {diff['correlation']:.4f}")
    else:
        print("‚úì WSZYSTKIE ≈πR√ìD≈ÅA SƒÑ ZGODNE (r√≥≈ºnice < 10 MW, korelacja > 0.99)")
    
    return differences_found


def compare_with_pse(df_csv, df_pse):
    """Por√≥wnuje dane z CSV z danymi z PSE API."""
    print("\n" + "="*80)
    print("üîå POR√ìWNANIE Z DANYMI PSE")
    print("="*80)
    
    if df_pse is None or df_pse.empty:
        print("‚ùå Brak danych z PSE do por√≥wnania")
        return
    
    print("\n‚ÑπÔ∏è  PSE dostarcza g≈Ç√≥wnie dane o wietrze i fotowoltaice")
    print("    Plik CSV zawiera dane ENTSO-E (wszystkie ≈∫r√≥d≈Ça)")
    print("    Por√≥wnanie ograniczone do dostƒôpnych kolumn")
    
    # PSE ma kolumny: Data, Wiatr, Fotowoltaika, ...
    pse_cols = list(df_pse.columns)
    print(f"\nKolumny PSE: {pse_cols}")
    
    # Mapowanie
    mappings = []
    if 'Wiatr' in df_pse.columns and 'wind_onshore' in df_csv.columns:
        mappings.append(('wind_onshore', 'Wiatr'))
    if 'Fotowoltaika' in df_pse.columns and 'solar' in df_csv.columns:
        mappings.append(('solar', 'Fotowoltaika'))
    
    if not mappings:
        print("‚ö†Ô∏è  Brak wsp√≥lnych kolumn do por√≥wnania")
        return
    
    # Przygotuj daty
    if 'Data' in df_pse.columns:
        df_pse['timestamp'] = pd.to_datetime(df_pse['Data'])
    else:
        df_pse['timestamp'] = df_pse.index
    
    for csv_col, pse_col in mappings:
        print(f"\nüìä {csv_col} vs {pse_col}:")
        
        # Znajd≈∫ wsp√≥lne daty
        csv_dates = set(df_csv['date_parsed'].dt.floor('H'))
        pse_dates = set(df_pse['timestamp'].dt.floor('H'))
        common_dates = csv_dates.intersection(pse_dates)
        
        if len(common_dates) == 0:
            print(f"   ‚ö†Ô∏è Brak wsp√≥lnych dat")
            continue
        
        print(f"   Wsp√≥lnych pomiar√≥w: {len(common_dates)}")
        
        # Merge
        df_csv_subset = df_csv[df_csv['date_parsed'].dt.floor('H').isin(common_dates)].copy()
        df_csv_subset['hour'] = df_csv_subset['date_parsed'].dt.floor('H')
        
        df_pse_subset = df_pse[df_pse['timestamp'].dt.floor('H').isin(common_dates)].copy()
        df_pse_subset['hour'] = df_pse_subset['timestamp'].dt.floor('H')
        
        merged = pd.merge(
            df_csv_subset[['hour', csv_col]],
            df_pse_subset[['hour', pse_col]],
            on='hour',
            how='inner'
        )
        
        if len(merged) > 0:
            merged['diff'] = merged[csv_col] - merged[pse_col]
            correlation = merged[csv_col].corr(merged[pse_col])
            
            print(f"   - ≈örednia r√≥≈ºnica: {merged['diff'].mean():.2f} MW")
            print(f"   - Maksymalna r√≥≈ºnica: {merged['diff'].abs().max():.2f} MW")
            print(f"   - Korelacja: {correlation:.6f}")


def analyze_time_consistency(df_csv):
    """Sprawdza ciƒÖg≈Ço≈õƒá czasowƒÖ i duplikaty."""
    print("\n" + "="*80)
    print("‚è∞ ANALIZA CIƒÑG≈ÅO≈öCI CZASOWEJ")
    print("="*80)
    
    if 'date_parsed' not in df_csv.columns:
        print("‚ùå Brak kolumny z datƒÖ")
        return
    
    df = df_csv.sort_values('date_parsed').copy()
    
    # Sprawd≈∫ duplikaty
    duplicates = df['date_parsed'].duplicated().sum()
    print(f"\n1. DUPLIKATY:")
    if duplicates > 0:
        print(f"   ‚ö†Ô∏è  Znaleziono {duplicates} zduplikowanych timestamp√≥w")
        dup_dates = df[df['date_parsed'].duplicated(keep=False)]['date_parsed'].unique()
        print(f"   Przyk≈Çady: {dup_dates[:5]}")
    else:
        print(f"   ‚úì Brak duplikat√≥w")
    
    # Sprawd≈∫ luki
    df['time_diff'] = df['date_parsed'].diff()
    expected_diff = pd.Timedelta(hours=1)
    
    gaps = df[df['time_diff'] != expected_diff]
    
    print(f"\n2. LUKI CZASOWE:")
    if len(gaps) > 1:  # Pierwsza r√≥≈ºnica zawsze bƒôdzie NaT
        print(f"   ‚ö†Ô∏è  Znaleziono {len(gaps)-1} luk w danych")
        for idx, row in gaps.head(10).iterrows():
            if pd.notna(row['time_diff']):
                print(f"   - {row['date_parsed']}: luka {row['time_diff']}")
    else:
        print(f"   ‚úì Brak luk (ciƒÖg≈Çe dane co 1 godzinƒô)")
    
    # Sprawd≈∫ zmiany czasu (DST)
    print(f"\n3. ZMIANY CZASU (DST):")
    dst_transitions = df[
        (df['time_diff'] == pd.Timedelta(hours=0)) |  # Powt√≥rzone godziny
        (df['time_diff'] == pd.Timedelta(hours=2))     # Pominiƒôte godziny
    ]
    
    if len(dst_transitions) > 0:
        print(f"   Znaleziono {len(dst_transitions)} zmian czasu:")
        for idx, row in dst_transitions.iterrows():
            transition_type = "Koniec DST (powt√≥rzona godzina)" if row['time_diff'] == pd.Timedelta(hours=0) else "PoczƒÖtek DST (pominiƒôta godzina)"
            print(f"   - {row['date_parsed']}: {transition_type}")
    else:
        print(f"   ‚ÑπÔ∏è  Brak wykrytych zmian czasu w pr√≥bce")


def generate_summary_report(df_csv, df_pse, df_entsoe):
    """Generuje podsumowanie por√≥wnania."""
    print("\n" + "="*80)
    print("üìã PODSUMOWANIE RAPORTU POR√ìWNAWCZEGO")
    print("="*80)
    
    print("\n1. ≈πR√ìD≈ÅO DANYCH - PLIK CSV:")
    print(f"   - Nazwa: electricity_production_entsoe_all (2).csv")
    print(f"   - Liczba rekord√≥w: {len(df_csv):,}")
    if 'date_parsed' in df_csv.columns:
        print(f"   - Zakres: {df_csv['date_parsed'].min()} - {df_csv['date_parsed'].max()}")
        years = (df_csv['date_parsed'].max() - df_csv['date_parsed'].min()).days / 365.25
        print(f"   - Okres: {years:.1f} lat")
    
    print("\n2. ≈πR√ìD≈ÅA DANYCH - API:")
    if df_pse is not None and not df_pse.empty:
        print(f"   ‚úì PSE: {len(df_pse)} rekord√≥w")
    else:
        print(f"   ‚úó PSE: brak danych")
    
    if df_entsoe is not None and not df_entsoe.empty:
        print(f"   ‚úì ENTSO-E: {len(df_entsoe)} rekord√≥w")
    else:
        print(f"   ‚úó ENTSO-E: brak danych")
    
    print("\n3. KLUCZOWE WNIOSKI:")
    print("   a) Struktura pliku CSV:")
    print("      - Zawiera dane godzinowe z ENTSO-E")
    print("      - Dwie kolumny czasowe: data lokalna i UTC")
    print("      - ~14 r√≥≈ºnych ≈∫r√≥de≈Ç energii")
    
    print("\n   b) Por√≥wnanie z danymi API:")
    print("      - Sprawd≈∫ sekcje powy≈ºej dla szczeg√≥≈Ç√≥w")
    print("      - Zwr√≥ƒá uwagƒô na r√≥≈ºnice w warto≈õciach")
    print("      - Sprawd≈∫ korelacjƒô miƒôdzy ≈∫r√≥d≈Çami")
    
    print("\n4. REKOMENDACJE:")
    print("   - Zweryfikuj ≈∫r√≥d≈Ço pliku CSV")
    print("   - Sprawd≈∫ metodologiƒô zbierania danych")
    print("   - W razie du≈ºych r√≥≈ºnic, u≈ºyj danych API jako referencji")
    print("   - Dokumentuj wszelkie rozbie≈ºno≈õci")


def main():
    """G≈Ç√≥wna funkcja programu."""
    print("="*80)
    print("üîç DOG≈ÅƒòBNE POR√ìWNANIE ≈πR√ìDE≈Å DANYCH - PRODUKCJA ENERGII")
    print("="*80)
    
    # ≈öcie≈ºka do pliku CSV
    csv_file = "/workspaces/produkcja-energii/electricity_production_entsoe_all (2).csv"
    
    # 1. Wczytaj i przeanalizuj CSV
    df_csv = load_csv_file(csv_file)
    energy_sources = analyze_csv_structure(df_csv)
    analyze_time_consistency(df_csv)
    
    # 2. Okre≈õl zakres do pobrania z API
    # U≈ºywamy pr√≥bkowania dla d≈Çugich okres√≥w
    if 'date_parsed' in df_csv.columns:
        min_date = df_csv['date_parsed'].min()
        max_date = df_csv['date_parsed'].max()
        
        # Dla bardzo d≈Çugich okres√≥w (>1 rok), we≈∫ tylko ostatni miesiƒÖc
        if (max_date - min_date).days > 365:
            print("\n" + "="*80)
            print("‚ÑπÔ∏è  Okres w pliku CSV przekracza 1 rok")
            print("   Do por√≥wnania u≈ºywam ostatniego miesiƒÖca danych")
            print("="*80)
            date_from = (max_date - timedelta(days=30)).strftime('%Y-%m-%d')
            date_to = max_date.strftime('%Y-%m-%d')
        else:
            date_from = min_date.strftime('%Y-%m-%d')
            date_to = max_date.strftime('%Y-%m-%d')
        
        # 3. Pobierz dane z API
        df_pse, df_entsoe = fetch_comparison_data(date_from, date_to)
        
        # 4. Por√≥wnaj dane
        compare_with_entsoe(df_csv, df_entsoe)
        compare_with_pse(df_csv, df_pse)
        
        # 5. Generuj raport
        generate_summary_report(df_csv, df_pse, df_entsoe)
    
    print("\n" + "="*80)
    print("‚úÖ RAPORT ZAKO≈ÉCZONY")
    print("="*80)


if __name__ == "__main__":
    main()
